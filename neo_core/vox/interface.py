"""
Vox ‚Äî Agent Interface Humaine
==============================
Cortex du langage et interface sociale du syst√®me Neo Core.

Responsabilit√©s :
- Interagir directement avec l'humain
- Reformuler et structurer les demandes avant transmission √† Brain
- Restituer les r√©ponses de Brain sans alt√©ration
- Fournir un √©tat en temps r√©el des autres agents
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Optional

from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from neo_core.config import NeoConfig, default_config, get_agent_model
from neo_core.memory.conversation import ConversationStore, ConversationSession
from neo_core.oauth import is_oauth_token
from neo_core.validation import validate_message, ValidationError

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from neo_core.brain.core import Brain
    from neo_core.memory.agent import MemoryAgent

VOX_SYSTEM_PROMPT = """Tu es Vox, l'interface de communication du syst√®me Neo Core.
Date et heure actuelles : {current_date}, {current_time}

Ton r√¥le :
- Tu es le point de contact entre l'humain et le syst√®me.
- Tu reformules les demandes de l'humain de mani√®re claire et structur√©e avant de les transmettre √† Brain.
- Tu restitues les r√©ponses de Brain de mani√®re naturelle et accessible.
- Tu peux r√©pondre aux questions simples toi-m√™me (salutations, conversation l√©g√®re).
- Tu peux informer l'humain de l'√©tat des autres agents si demand√©.

Ce que Neo Core peut faire (informe l'utilisateur s'il demande) :
- Chercher sur internet (actualit√©s, m√©t√©o, prix, scores...)
- √âcrire et ex√©cuter du code (Python sandbox, debug, analyse)
- Lire et √©crire des fichiers
- R√©diger des textes (articles, emails, rapports)
- Traduire dans toutes les langues
- Analyser des donn√©es et identifier des tendances
- Cr√©er des t√¢ches et des projets (missions multi-√©tapes)
- M√©moriser les pr√©f√©rences et apprendre des interactions
- Utiliser des plugins personnalis√©s

R√®gles :
- Sois naturel, empathique et efficace dans ta communication.
- Ne modifie jamais le fond des r√©ponses de Brain, seulement la forme si n√©cessaire.
- Si une demande est ambigu√´, pose des questions de clarification.
- Quand Brain travaille sur une t√¢che longue, tu peux continuer √† discuter avec l'humain.
- R√©ponds de mani√®re concise et naturelle, pas de markdown excessif.

{personality}

√âtat actuel du syst√®me :
{system_status}
"""

# Prompt pour la reformulation intelligente des requ√™tes
VOX_REFORMULATE_PROMPT = """Tu es Vox, l'interface du syst√®me Neo Core.
Reformule cette demande utilisateur en une requ√™te claire et structur√©e pour Brain (l'orchestrateur).

{recent_context}Demande originale : {user_message}

R√®gles :
- Conserve l'intention exacte de l'utilisateur
- Si la demande est courte ou utilise des pronoms ("√ßa", "pareil", "continue"),
  r√©sous les r√©f√©rences en utilisant le contexte r√©cent ci-dessus
- Clarifie les ambigu√Øt√©s si possible
- Structure la demande pour faciliter le travail de Brain
- Reste concis (1-3 phrases max)
- Si la demande est d√©j√† claire, retourne-la telle quelle
- R√©ponds UNIQUEMENT avec la requ√™te reformul√©e, rien d'autre.
"""

# Acks statiques pour accus√© de r√©ception instantan√©
STATIC_ACKS = [
    "Je transmets √† Brain, un instant...",
    "Compris, Brain analyse votre demande...",
    "Bien re√ßu, je traite votre requ√™te...",
]

# Liste UNIQUE de pronoms/expressions ambigus n√©cessitant contexte pour d√©sambigu√Øser
# Utilis√©e √† la fois par process_message() (skip reformulation) et format_request_async()
_AMBIGUOUS_PRONOUNS = frozenset([
    "√ßa", "cela", "ce", "il", "elle", "les", "eux", "celle", "celui",
    "pareil", "m√™me", "m√™me chose", "le m√™me", "la m√™me", "les m√™mes",
    "continue", "encha√Æne", "enchaine", "la suite",
    "fais-le", "fais le", "fais-la", "fais la",
])


@dataclass
class AgentStatus:
    """√âtat d'un agent dans le syst√®me."""
    name: str
    active: bool = False
    current_task: Optional[str] = None
    progress: float = 0.0

    def to_string(self) -> str:
        status = "actif" if self.active else "inactif"
        task = self.current_task or "aucune t√¢che"
        return f"[{self.name}] {status} ‚Äî {task} ({self.progress:.0%})"


@dataclass
class Vox:
    """
    Agent Vox ‚Äî Interface humaine du syst√®me Neo Core.

    Agit comme le cortex du langage : re√ßoit les messages humains,
    les structure, les transmet √† Brain, et restitue les r√©ponses.

    Poss√®de son propre LLM (Haiku) pour :
    - Reformuler les requ√™tes avant transmission √† Brain
    - Restituer les r√©ponses de Brain de mani√®re naturelle
    - G√©n√©rer des accus√©s de r√©ception instantan√©s
    """
    config: NeoConfig = field(default_factory=lambda: default_config)
    brain: Optional[Brain] = None
    memory: Optional[MemoryAgent] = None
    conversation_history: list = field(default_factory=list)
    _agent_statuses: dict[str, AgentStatus] = field(default_factory=dict)
    _llm: Optional[object] = None
    _force_mock: bool = False
    _model_config: Optional[object] = None
    _on_thinking_callback: Optional[object] = None  # Callable[[str], None]
    _on_brain_done_callback: Optional[object] = None  # Callable[[str], None] ‚Äî async brain result
    _conversation_store: Optional[ConversationStore] = None
    _current_session: Optional[ConversationSession] = None
    _session_lock: Optional[object] = None  # threading.Lock
    _brain_task: Optional[object] = None  # asyncio.Task ‚Äî t√¢che Brain en cours
    _brain_busy: bool = False  # True quand Brain travaille en arri√®re-plan

    def __post_init__(self):
        self._agent_statuses = {
            "Vox": AgentStatus(name="Vox", active=True, current_task="communication"),
            "Brain": AgentStatus(name="Brain"),
            "Memory": AgentStatus(name="Memory"),
        }
        self._model_config = get_agent_model("vox")

        # Lock pour prot√©ger _current_session contre les acc√®s concurrents
        import threading
        self._session_lock = threading.Lock()

        # Initialiser le conversation store (chemin relatif au data_dir de la config)
        db_path = self.config.memory.storage_path / "conversations.db"
        self._conversation_store = ConversationStore(db_path)

        if not self.config.is_mock_mode():
            self._init_llm()

    @property
    def _mock_mode(self) -> bool:
        """V√©rifie dynamiquement si Vox est en mode mock.

        Re-v√©rifie la config √† chaque appel. Si la cl√© API devient
        disponible apr√®s le d√©marrage, Vox s'auto-initialise.
        """
        if self._force_mock:
            return True
        if self.config.is_mock_mode():
            return True
        # La cl√© est disponible mais le LLM n'est pas encore initialis√©
        if self._llm is None and not is_oauth_token(self.config.llm.api_key or ""):
            try:
                self._init_llm()
            except Exception:
                pass  # Vox fonctionne en passthrough sans LLM
        return False

    def _init_llm(self) -> None:
        """Initialise le LLM d√©di√© de Vox (Haiku ‚Äî rapide et l√©ger)."""
        try:
            api_key = self.config.llm.api_key
            if is_oauth_token(api_key):
                # En mode OAuth, Vox utilise les appels directs httpx
                self._llm = None  # Sera g√©r√© via _vox_llm_call
            else:
                from langchain_anthropic import ChatAnthropic
                self._llm = ChatAnthropic(
                    model=self._model_config.model,
                    api_key=api_key,
                    temperature=self._model_config.temperature,
                    max_tokens=self._model_config.max_tokens,
                )
            logger.info("LLM initialis√© : %s", self._model_config.model)
        except Exception as e:
            logger.error("LLM non disponible (%s), mode passthrough", e)

    async def _vox_llm_call(self, prompt: str) -> str:
        """
        Appel LLM d√©di√© pour Vox (reformulation/restitution).

        Route via le syst√®me multi-provider (Ollama, Groq, Gemini, Anthropic).
        Fallback automatique vers Anthropic direct si aucun provider configur√©.
        """
        if self._mock_mode:
            return prompt  # Passthrough en mock

        try:
            from neo_core.brain.providers.router import route_chat

            response = await route_chat(
                agent_name="vox",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self._model_config.max_tokens,
                temperature=self._model_config.temperature,
            )

            if response.text and not response.text.startswith("[Erreur"):
                return response.text
            # Fallback : retourne le prompt original
            return prompt

        except Exception as e:
            # Fallback LangChain legacy
            logger.debug("route_chat failed, trying legacy LangChain: %s", e)
            if self._llm:
                try:
                    result = await self._llm.ainvoke(prompt)
                    return result.content
                except Exception as e:
                    logger.debug("LangChain ainvoke failed: %s", e)

        return prompt  # Passthrough si pas de LLM

    def connect(self, brain: Brain, memory: MemoryAgent) -> None:
        """Connecte Vox aux autres agents du Neo Core."""
        self.brain = brain
        self.memory = memory

    def start_new_session(self, user_name: str = "User") -> ConversationSession:
        """D√©marre une nouvelle session de conversation."""
        if not self._conversation_store:
            return None
        with self._session_lock:
            self._current_session = self._conversation_store.start_session(user_name)
            logger.info("Started new conversation session: %s", self._current_session.session_id)
            return self._current_session

    def resume_session(self, session_id: str) -> Optional[ConversationSession]:
        """Charge une session existante et restaure son historique (thread-safe)."""
        if not self._conversation_store:
            return None
        with self._session_lock:
            session = self._conversation_store.get_session_by_id(session_id)
            if not session:
                logger.warning("Session not found: %s", session_id)
                return None

            # Charger l'historique
            history = self._conversation_store.get_history(session_id, limit=10000)
            self.conversation_history = []
            for turn in history:
                if turn.role == "human":
                    self.conversation_history.append(HumanMessage(content=turn.content))
                else:
                    self.conversation_history.append(AIMessage(content=turn.content))

            self._current_session = session
            logger.info("Resumed session: %s with %d messages", session_id, len(history))
            return self._current_session

    def get_session_info(self) -> Optional[dict]:
        """Retourne les infos de la session courante."""
        if not self._current_session:
            return None
        return {
            "session_id": self._current_session.session_id,
            "user_name": self._current_session.user_name,
            "created_at": self._current_session.created_at,
            "updated_at": self._current_session.updated_at,
            "message_count": self._current_session.message_count,
        }

    def set_thinking_callback(self, callback) -> None:
        """
        D√©finit un callback appel√© quand Vox envoie un ack
        pendant que Brain r√©fl√©chit.

        Le callback re√ßoit un str (message d'accus√© de r√©ception).
        """
        self._on_thinking_callback = callback

    def set_brain_done_callback(self, callback) -> None:
        """
        D√©finit un callback appel√© quand Brain termine en arri√®re-plan.
        Le callback re√ßoit un str (r√©ponse de Brain).
        Utilis√© en mode asynchrone pour d√©livrer le r√©sultat au CLI.
        """
        self._on_brain_done_callback = callback

    def push_message(self, message: str, source: str = "brain") -> None:
        """
        Envoie un message proactif √† l'utilisateur sur tous les canaux.

        Appel√© par le heartbeat quand Brain d√©cide de parler spontan√©ment.
        Route vers : TUI (callback), Telegram, et stockage m√©moire.
        """
        import logging as _log
        _logger = _log.getLogger(__name__)

        # 1. TUI / frontend via callback
        if self._on_brain_done_callback:
            try:
                self._on_brain_done_callback(f"üß† {message}")
            except Exception as e:
                _logger.debug("push_message callback failed: %s", e)

        # 2. Telegram
        try:
            from neo_core.infra.registry import core_registry
            core_registry.send_telegram(f"üß† {message}")
        except Exception as e:
            _logger.debug("push_message telegram failed: %s", e)

        # 3. Stockage m√©moire (Brain se souvient de ce qu'il a dit)
        if self.memory and self.memory.is_initialized:
            try:
                self.memory.store_memory(
                    content=f"[Proactif ‚Äî {source}] {message}",
                    source="brain_proactive",
                    tags=["proactive", source],
                    importance=0.6,
                )
            except Exception as e:
                _logger.debug("push_message memory store failed: %s", e)

        _logger.info("[Vox] Message proactif envoy√©: %s", message[:80])

    @property
    def is_brain_busy(self) -> bool:
        """Indique si Brain travaille en arri√®re-plan."""
        return self._brain_busy

    def _is_simple_message(self, message: str) -> bool:
        """
        D√©termine si un message peut √™tre trait√© par Vox seul (sans Brain).

        Seuls deux cas restent en fast-path Vox :
        1. Commandes slash (/status, /tasks, etc.) ‚Äî routage structurel
        2. Salutations pures isol√©es ("salut", "bonjour", "merci")

        Tout le reste est envoy√© √† Brain pour classification LLM.
        C'est le LLM qui d√©cide, pas des heuristiques.
        """
        msg_lower = message.lower().strip()

        # Commandes slash ‚Üí routage structurel, pas besoin de Brain
        if msg_lower.startswith("/"):
            return True

        # Salutations pures isol√©es (le message n'est QUE √ßa, sans contexte)
        import re as _re
        pure_greetings = (
            r"^(salut|hello|bonjour|bonsoir|hey|hi|yo|coucou|wesh|slt|"
            r"√ßa va|ca va|quoi de neuf|"
            r"merci|thanks)"
            r"\s*[!.\?]*$"
        )
        if _re.match(pure_greetings, msg_lower):
            return True

        # Tout le reste ‚Üí Brain (le LLM d√©cide)
        return False

    async def _vox_quick_reply(self, message: str) -> str:
        """
        G√©n√®re une r√©ponse rapide de Vox (sans Brain) pour les messages simples.
        Si Brain travaille en arri√®re-plan, mentionne-le.
        """
        brain_status = ""
        if self._brain_busy:
            brain_status = "\n(Brain travaille encore sur ta demande pr√©c√©dente, je te donnerai la r√©ponse d√®s qu'elle est pr√™te.)"

        if self._mock_mode:
            return f"Salut ! Je suis Vox, l'interface de Neo Core. Je suis l√† pour t'aider !{brain_status}"

        try:
            import asyncio
            prompt = (
                f"Tu es Vox, l'interface de Neo Core. R√©ponds de mani√®re courte et naturelle "
                f"√† ce message de l'utilisateur (max 2-3 phrases). "
                f"Si on te demande tes capacit√©s, explique ce que Neo peut faire : "
                f"chercher sur internet, √©crire du code, analyser des donn√©es, r√©diger des textes, "
                f"traduire, cr√©er des t√¢ches/projets, m√©moriser et apprendre.\n\n"
                f"Message : {message}\n\n"
                f"{'Note: Brain travaille actuellement sur une t√¢che en arri√®re-plan.' if self._brain_busy else ''}"
            )
            reply = await asyncio.wait_for(
                self._vox_llm_call(prompt),
                timeout=5.0,
            )
            return reply.strip() + brain_status if reply else f"Je suis l√† !{brain_status}"
        except Exception:
            return f"Je suis l√†, comment puis-je t'aider ?{brain_status}"

    async def _generate_ack(self, user_message: str) -> str:
        """
        G√©n√®re un accus√© de r√©ception INSTANTAN√â (pas d'appel LLM).

        L'ACK doit √™tre imm√©diat (<10ms) pour que le prompt revienne
        tout de suite. Brain tourne en fond, le r√©sultat arrivera via callback.
        """
        import random

        # ACK contextuel bas√© sur des mots-cl√©s simples (pas de LLM)
        msg_lower = user_message.lower()

        if any(w in msg_lower for w in ("cherche", "recherche", "trouve", "search", "look")):
            return random.choice(["üîç Je lance la recherche...", "Je cherche √ßa...", "Recherche en cours..."])
        if any(w in msg_lower for w in ("code", "script", "programme", "debug", "fix")):
            return random.choice(["üíª Je m'en occupe...", "Je travaille dessus...", "Code en cours..."])
        if any(w in msg_lower for w in ("cr√©e", "cr√©er", "monte", "lance", "projet", "project")):
            return random.choice(["üìã Je mets √ßa en place...", "Projet en pr√©paration...", "Je structure √ßa..."])
        if any(w in msg_lower for w in ("√©cris", "r√©dige", "traduis", "r√©sume")):
            return random.choice(["‚úçÔ∏è J'y travaille...", "R√©daction en cours...", "Je m'en charge..."])
        if any(w in msg_lower for w in ("analyse", "calcule", "compare", "√©value")):
            return random.choice(["üìä Analyse en cours...", "Je calcule...", "Je regarde les donn√©es..."])

        return random.choice(STATIC_ACKS)

    def _get_personality_injection(self) -> str:
        """R√©cup√®re l'injection de personnalit√© depuis le PersonaEngine."""
        if self.memory and self.memory.persona_engine and self.memory.persona_engine.is_initialized:
            try:
                return self.memory.persona_engine.get_vox_injection()
            except Exception as e:
                logger.debug("Failed to get personality injection: %s", e)
        return ""

    def get_system_status(self) -> str:
        """G√©n√®re un r√©sum√© de l'√©tat de tous les agents."""
        lines = [status.to_string() for status in self._agent_statuses.values()]
        return "\n".join(lines)

    def update_agent_status(self, name: str, active: bool = False,
                            task: Optional[str] = None, progress: float = 0.0) -> None:
        """Met √† jour le statut d'un agent."""
        if name in self._agent_statuses:
            self._agent_statuses[name].active = active
            self._agent_statuses[name].current_task = task
            self._agent_statuses[name].progress = progress

    # ‚îÄ‚îÄ Gestion intelligente de l'historique ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    _MAX_HISTORY_TOKENS = 12_000   # budget tokens pour l'historique conversation
    _SUMMARY_TRIGGER = 14_000      # seuil pour d√©clencher la compression
    _CHARS_PER_TOKEN = 4           # estimation fr/en (conservateur)

    def _estimate_tokens(self, text: str) -> int:
        """Estimation rapide du nombre de tokens (~4 chars/token)."""
        return len(text) // self._CHARS_PER_TOKEN

    def _history_token_count(self) -> int:
        """Compte les tokens estim√©s dans l'historique conversation."""
        return sum(self._estimate_tokens(m.content) for m in self.conversation_history)

    def _trim_history_by_tokens(self) -> None:
        """
        Gestion intelligente de l'historique bas√©e sur un budget tokens.
        Au lieu de couper brutalement √† 50 messages :
        1. Si < _MAX_HISTORY_TOKENS ‚Üí on garde tout
        2. Si > _SUMMARY_TRIGGER ‚Üí on r√©sume les vieux messages et on ne garde
           que le r√©sum√© + les messages r√©cents
        """
        total = self._history_token_count()
        if total <= self._MAX_HISTORY_TOKENS:
            return  # tout tient dans le budget

        # Strat√©gie : garder les 20 derniers messages intacts,
        # r√©sumer tout ce qui pr√©c√®de en un seul message "contexte"
        keep_recent = 20
        if len(self.conversation_history) <= keep_recent:
            return  # pas assez de messages pour compresser

        old_messages = self.conversation_history[:-keep_recent]
        recent_messages = self.conversation_history[-keep_recent:]

        # Construire un r√©sum√© condens√© des vieux messages
        summary_parts = []
        for msg in old_messages:
            role = "User" if isinstance(msg, HumanMessage) else "Neo"
            # Garder les 200 premiers chars de chaque message
            content = msg.content[:200]
            if len(msg.content) > 200:
                content += "..."
            summary_parts.append(f"{role}: {content}")

        summary_text = (
            "[R√©sum√© des √©changes pr√©c√©dents]\n"
            + "\n".join(summary_parts[-30:])  # max 30 entr√©es dans le r√©sum√©
        )

        # Remplacer l'historique : r√©sum√© + messages r√©cents
        self.conversation_history = [
            AIMessage(content=summary_text),
            *recent_messages,
        ]
        logger.info(
            "[VOX] History compressed: %d messages ‚Üí summary + %d recent (was %d tokens, now ~%d)",
            len(old_messages), len(recent_messages), total, self._history_token_count()
        )

    def format_request(self, human_message: str) -> str:
        """
        Reformule et structure la demande humaine.
        Version synchrone (fallback) ‚Äî retourne tel quel.
        """
        return human_message

    def _build_recent_context(self, max_turns: int = 8) -> str:
        """
        Construit un r√©sum√© des derniers √©changes pour contextualiser
        les messages courts ou ambigus (pronoms, r√©f√©rences implicites).
        """
        if not self.conversation_history:
            return ""

        # Prendre les derniers √©changes (max_turns * 2 messages)
        recent = self.conversation_history[-(max_turns * 2):]
        if not recent:
            return ""

        lines = ["Contexte r√©cent de la conversation :"]
        for msg in recent:
            role = "User" if isinstance(msg, HumanMessage) else "Neo"
            # Tronquer les r√©ponses longues pour ne pas saturer le prompt
            content = msg.content[:400] + "..." if len(msg.content) > 400 else msg.content
            lines.append(f"  {role}: {content}")
        lines.append("")  # Ligne vide avant la demande
        return "\n".join(lines)

    async def format_request_async(self, human_message: str) -> str:
        """
        Reformule intelligemment la demande via le LLM de Vox (Haiku).
        Injecte le contexte r√©cent pour r√©soudre les pronoms et r√©f√©rences.
        Si le LLM n'est pas disponible, retourne le message tel quel.
        """
        if self._mock_mode or (not self._llm and not is_oauth_token(self.config.llm.api_key or "")):
            return human_message

        try:
            # Injecter le contexte r√©cent pour les messages courts/ambigus ou avec pronoms
            recent_context = ""
            msg_lower = human_message.lower()
            _has_pronoun = any(p in msg_lower for p in _AMBIGUOUS_PRONOUNS)
            if (len(human_message.split()) < 20 or _has_pronoun) and self.conversation_history:
                recent_context = self._build_recent_context()

            prompt = VOX_REFORMULATE_PROMPT.format(
                user_message=human_message,
                recent_context=recent_context,
            )
            result = await self._vox_llm_call(prompt)
            return result.strip() if result else human_message
        except Exception as e:
            logger.debug("Failed to reformat request: %s", e)
            return human_message

    async def process_message(self, human_message: str) -> str:
        """
        Pipeline principal : humain ‚Üí Vox(LLM) ‚Üí Brain(LLM) ‚Üí humain.

        Mode synchrone (par d√©faut) :
        1. Si message simple ‚Üí Vox r√©pond directement (sans Brain)
        2. Si message complexe ‚Üí Vox reformule ‚Üí Brain traite ‚Üí retourne r√©ponse

        Mode asynchrone (si _on_brain_done_callback est d√©fini) :
        1. Si message simple ‚Üí Vox r√©pond imm√©diatement
        2. Si message complexe ‚Üí Lance Brain en arri√®re-plan, retourne un ack
        3. Quand Brain termine ‚Üí callback avec la r√©ponse
        """
        # Input validation
        try:
            human_message = validate_message(human_message)
        except ValidationError as e:
            logger.error("Message validation failed: %s", e)
            return f"[Erreur] Message invalide : {e}"

        if not self.brain:
            return "[Erreur] Brain n'est pas connect√© √† Vox."

        # Enregistre le message dans l'historique (budget tokens, pas message count)
        self.conversation_history.append(HumanMessage(content=human_message))
        self._trim_history_by_tokens()

        # ‚îÄ‚îÄ Slash commands ‚Üí routage structurel sans Brain ‚îÄ‚îÄ
        if human_message.strip().startswith("/"):
            # Les commandes slash sont g√©r√©es par le legacy pipeline de Brain
            pass  # Tombe dans le flux normal ci-dessous

        # ‚îÄ‚îÄ TOUT passe √† Brain ‚Äî plus de reformulation, plus de Vox LLM ‚îÄ‚îÄ
        # Brain (Sonnet) a l'historique complet, il comprend "√ßa", "pareil", "salut"
        # Z√©ro interm√©diaire = z√©ro perte d'intelligence
        formatted_request = human_message

        # Mode asynchrone : lancer Brain en arri√®re-plan
        if self._on_brain_done_callback:
            import asyncio

            # Accus√© de r√©ception
            ack = await self._generate_ack(human_message)
            self.update_agent_status("Vox", active=False, task="communication", progress=0.0)
            self.update_agent_status("Brain", active=True, task="analyse de la requ√™te", progress=0.5)

            # Lancer Brain en arri√®re-plan
            self._brain_busy = True
            self._brain_task = asyncio.create_task(
                self._brain_background(formatted_request, human_message)
            )

            return ack

        # Mode synchrone (fallback / API / Telegram) : attendre Brain
        if self._on_thinking_callback:
            try:
                ack = await self._generate_ack(human_message)
                self._on_thinking_callback(ack)
            except Exception as e:
                logger.debug("Failed to send thinking callback: %s", e)

        self.update_agent_status("Vox", active=False, task="communication", progress=0.0)
        self.update_agent_status("Brain", active=True, task="analyse de la requ√™te", progress=0.5)

        brain_response = await self.brain.process(
            request=formatted_request,
            conversation_history=self.conversation_history,
            original_request=human_message,
        )

        self.update_agent_status("Brain", active=False, task=None, progress=0.0)
        final_response = brain_response

        self.conversation_history.append(AIMessage(content=final_response))
        self._save_conversation_turn(human_message, final_response)
        self._store_in_memory(human_message, final_response)

        return final_response

    async def _brain_background(self, formatted_request: str, original_message: str) -> None:
        """Ex√©cute Brain en arri√®re-plan et d√©livre le r√©sultat via callback."""
        try:
            brain_response = await self.brain.process(
                request=formatted_request,
                conversation_history=self.conversation_history,
                original_request=original_message,
            )

            self.update_agent_status("Brain", active=False, task=None, progress=0.0)

            # Enregistrer dans l'historique
            self.conversation_history.append(AIMessage(content=brain_response))
            self._save_conversation_turn(original_message, brain_response)
            self._store_in_memory(original_message, brain_response)

            # D√©livrer via callback
            if self._on_brain_done_callback:
                self._on_brain_done_callback(brain_response)

        except Exception as e:
            logger.error("Brain background task failed: %s", e)
            error_msg = f"[Erreur Brain] {type(e).__name__}: {str(e)[:300]}"
            if self._on_brain_done_callback:
                self._on_brain_done_callback(error_msg)
        finally:
            self._brain_busy = False
            self._brain_task = None

    def _save_conversation_turn(self, human_message: str, response: str) -> None:
        """Sauvegarde un √©change dans le ConversationStore."""
        if self._conversation_store and self._current_session:
            try:
                self._conversation_store.append_turn(
                    self._current_session.session_id,
                    "human",
                    human_message,
                )
                self._conversation_store.append_turn(
                    self._current_session.session_id,
                    "assistant",
                    response,
                )
            except Exception as e:
                logger.error("Failed to save conversation turn: %s", e)

    def _store_in_memory(self, human_message: str, response: str) -> None:
        """Stocke l'√©change en m√©moire persistante et met √† jour la working memory."""
        if self.memory and self.memory.is_initialized:
            self.update_agent_status("Memory", active=True, task="stockage", progress=0.5)
            self.memory.on_conversation_turn(human_message, response)
            self.update_agent_status("Memory", active=False, task=None, progress=0.0)

    def get_prompt_template(self) -> ChatPromptTemplate:
        """Retourne le template de prompt pour Vox avec personnalit√© inject√©e."""
        personality = self._get_personality_injection()
        prompt = VOX_SYSTEM_PROMPT.replace("{personality}", personality)
        return ChatPromptTemplate.from_messages([
            ("system", prompt),
            MessagesPlaceholder("conversation_history"),
            ("human", "{input}"),
        ])

    async def generate_session_summary(self) -> None:
        """
        G√©n√®re un r√©sum√© de la session courante via LLM Haiku.

        Appel√© √† la fin de chaque session (quit, Ctrl+C).
        Le r√©sum√© est stock√© dans ConversationStore.session_summaries.
        Skipp√© si la session a < 3 messages.
        """
        if not self._conversation_store or not self._current_session:
            return

        session_id = self._current_session.session_id
        session = self._conversation_store.get_session_by_id(session_id)
        if not session or session.message_count < 3:
            logger.debug("Session too short for summary (%d messages)", session.message_count if session else 0)
            return

        try:
            import asyncio

            # R√©cup√©rer les turns format√©s
            turns_text = self._conversation_store.get_session_turns_for_summary(session_id)
            if not turns_text:
                return

            summary_prompt = f"""R√©sume cette conversation en 2-3 phrases concises.
Extrais aussi les sujets cl√©s, les d√©cisions prises, et les faits importants appris sur l'utilisateur.

Conversation :
{turns_text[:3000]}

R√©ponds UNIQUEMENT en JSON :
{{"summary": "r√©sum√© concis de la conversation", "key_topics": ["sujet1", "sujet2"], "key_decisions": ["d√©cision1"], "key_facts": ["fait1"]}}"""

            # Appel Haiku rapide (timeout 5s)
            response_text = await asyncio.wait_for(
                self._vox_llm_call(summary_prompt),
                timeout=5.0,
            )

            if response_text:
                import json
                # Parser le JSON (tol√©rant aux backticks markdown)
                cleaned = response_text.strip()
                if cleaned.startswith("```"):
                    cleaned = cleaned.split("\n", 1)[1] if "\n" in cleaned else cleaned[3:]
                    if cleaned.endswith("```"):
                        cleaned = cleaned[:-3]
                    cleaned = cleaned.strip()

                data = json.loads(cleaned)
                self._conversation_store.save_session_summary(
                    session_id=session_id,
                    summary=data.get("summary", ""),
                    key_topics=data.get("key_topics", []),
                    key_decisions=data.get("key_decisions", []),
                    key_facts=data.get("key_facts", []),
                )
                logger.info("[Vox] Session summary generated for %s", session_id[:8])

        except Exception as e:
            logger.warning("[Vox] Failed to generate session summary: %s", e)

    def get_model_info(self) -> dict:
        """Retourne les infos du mod√®le utilis√© par Vox."""
        return {
            "agent": "Vox",
            "model": self._model_config.model if self._model_config else "none",
            "role": "Interface utilisateur (reformulation/restitution)",
            "has_llm": self._llm is not None or (
                not self._mock_mode and is_oauth_token(self.config.llm.api_key or "")
            ),
        }


def _warmup_providers() -> None:
    """
    Warm-up des providers LLM au d√©marrage.

    Envoie un appel minimal √† Haiku pour ouvrir la connexion r√©seau.
    √áa √©vite le cold start sur la premi√®re vraie requ√™te utilisateur,
    qui ferait timeout le classifier (8s).
    """
    import asyncio

    async def _ping():
        try:
            from neo_core.brain.providers.router import route_chat
            await asyncio.wait_for(
                route_chat(
                    agent_name="vox",
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=5,
                    temperature=0.0,
                ),
                timeout=10.0,
            )
            logger.info("[Warmup] Provider connection established")
        except Exception as e:
            logger.warning("[Warmup] Provider warmup failed (non-blocking): %s", e)

    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # On est d√©j√† dans une boucle async ‚Üí lancer en background
            asyncio.ensure_future(_ping())
        else:
            loop.run_until_complete(_ping())
    except RuntimeError:
        # Pas de loop ‚Üí en cr√©er une
        asyncio.run(_ping())


def bootstrap() -> Vox:
    """
    Initialise et connecte les 3 agents du Neo Core.
    Retourne l'agent Vox pr√™t √† communiquer.
    """
    from neo_core.brain.core import Brain
    from neo_core.memory.agent import MemoryAgent

    config = NeoConfig()

    memory = MemoryAgent(config=config)
    memory.initialize()

    brain = Brain(config=config)
    brain.connect_memory(memory)

    vox = Vox(config=config)
    vox.connect(brain=brain, memory=memory)

    # Warm-up : ouvre la connexion provider pour √©viter le cold start
    if not config.is_mock_mode():
        _warmup_providers()

    return vox
