"""
Vox â€” Agent Interface Humaine
==============================
Cortex du langage et interface sociale du systÃ¨me Neo Core.

ResponsabilitÃ©s :
- Interagir directement avec l'humain
- Reformuler et structurer les demandes avant transmission Ã  Brain
- Restituer les rÃ©ponses de Brain sans altÃ©ration
- Fournir un Ã©tat en temps rÃ©el des autres agents
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING, Optional

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from neo_core.config import NeoConfig, default_config, get_agent_model
from neo_core.memory.conversation import ConversationStore, ConversationSession
from neo_core.oauth import is_oauth_token, get_valid_access_token, OAUTH_BETA_HEADER
from neo_core.validation import validate_message, ValidationError

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from neo_core.brain.core import Brain
    from neo_core.memory.agent import MemoryAgent

VOX_SYSTEM_PROMPT = """Tu es Vox, l'interface de communication du systÃ¨me Neo Core.
Date et heure actuelles : {current_date}, {current_time}

Ton rÃ´le :
- Tu es le point de contact entre l'humain et le systÃ¨me.
- Tu reformules les demandes de l'humain de maniÃ¨re claire et structurÃ©e avant de les transmettre Ã  Brain.
- Tu restitues les rÃ©ponses de Brain de maniÃ¨re naturelle et accessible.
- Tu peux rÃ©pondre aux questions simples toi-mÃªme (salutations, conversation lÃ©gÃ¨re).
- Tu peux informer l'humain de l'Ã©tat des autres agents si demandÃ©.

Ce que Neo Core peut faire (informe l'utilisateur s'il demande) :
- Chercher sur internet (actualitÃ©s, mÃ©tÃ©o, prix, scores...)
- Ã‰crire et exÃ©cuter du code (Python sandbox, debug, analyse)
- Lire et Ã©crire des fichiers
- RÃ©diger des textes (articles, emails, rapports)
- Traduire dans toutes les langues
- Analyser des donnÃ©es et identifier des tendances
- CrÃ©er des tÃ¢ches et des projets (missions multi-Ã©tapes)
- MÃ©moriser les prÃ©fÃ©rences et apprendre des interactions
- Utiliser des plugins personnalisÃ©s

RÃ¨gles :
- Sois naturel, empathique et efficace dans ta communication.
- Ne modifie jamais le fond des rÃ©ponses de Brain, seulement la forme si nÃ©cessaire.
- Si une demande est ambiguÃ«, pose des questions de clarification.
- Quand Brain travaille sur une tÃ¢che longue, tu peux continuer Ã  discuter avec l'humain.
- RÃ©ponds de maniÃ¨re concise et naturelle, pas de markdown excessif.

{personality}

Ã‰tat actuel du systÃ¨me :
{system_status}
"""

# Prompt pour la reformulation intelligente des requÃªtes
VOX_REFORMULATE_PROMPT = """Tu es Vox, l'interface du systÃ¨me Neo Core.
Reformule cette demande utilisateur en une requÃªte claire et structurÃ©e pour Brain (l'orchestrateur).

{recent_context}Demande originale : {user_message}

RÃ¨gles :
- Conserve l'intention exacte de l'utilisateur
- Si la demande est courte ou utilise des pronoms ("Ã§a", "pareil", "continue"),
  rÃ©sous les rÃ©fÃ©rences en utilisant le contexte rÃ©cent ci-dessus
- Clarifie les ambiguÃ¯tÃ©s si possible
- Structure la demande pour faciliter le travail de Brain
- Reste concis (1-3 phrases max)
- Si la demande est dÃ©jÃ  claire, retourne-la telle quelle
- RÃ©ponds UNIQUEMENT avec la requÃªte reformulÃ©e, rien d'autre.
"""

# Prompt pour l'accusÃ© de rÃ©ception instantanÃ©
VOX_ACK_PROMPT = """Tu es Vox, l'interface du systÃ¨me Neo Core.
L'utilisateur vient d'envoyer un message. GÃ©nÃ¨re un court accusÃ© de rÃ©ception
(max 15 mots) pour lui dire que tu as compris et que Brain travaille dessus.

Message : {user_message}

RÃ¨gles :
- Sois naturel et rassurant
- Maximum 15 mots
- Pas de markdown, pas d'emojis
- Montre que tu as compris le sujet
- RÃ©ponds UNIQUEMENT avec l'accusÃ© de rÃ©ception, rien d'autre.
"""

# Acks statiques (fallback si LLM Ã©choue)
STATIC_ACKS = [
    "Je transmets Ã  Brain, un instant...",
    "Compris, Brain analyse votre demande...",
    "Bien reÃ§u, je traite votre requÃªte...",
]

# Liste UNIQUE de pronoms/expressions ambigus nÃ©cessitant contexte pour dÃ©sambiguÃ¯ser
# UtilisÃ©e Ã  la fois par process_message() (skip reformulation) et format_request_async()
_AMBIGUOUS_PRONOUNS = frozenset([
    "Ã§a", "cela", "ce", "il", "elle", "les", "eux", "celle", "celui",
    "pareil", "mÃªme", "mÃªme chose", "le mÃªme", "la mÃªme", "les mÃªmes",
    "continue", "enchaÃ®ne", "enchaine", "la suite",
    "fais-le", "fais le", "fais-la", "fais la",
])


@dataclass
class AgentStatus:
    """Ã‰tat d'un agent dans le systÃ¨me."""
    name: str
    active: bool = False
    current_task: Optional[str] = None
    progress: float = 0.0

    def to_string(self) -> str:
        status = "actif" if self.active else "inactif"
        task = self.current_task or "aucune tÃ¢che"
        return f"[{self.name}] {status} â€” {task} ({self.progress:.0%})"


@dataclass
class Vox:
    """
    Agent Vox â€” Interface humaine du systÃ¨me Neo Core.

    Agit comme le cortex du langage : reÃ§oit les messages humains,
    les structure, les transmet Ã  Brain, et restitue les rÃ©ponses.

    PossÃ¨de son propre LLM (Haiku) pour :
    - Reformuler les requÃªtes avant transmission Ã  Brain
    - Restituer les rÃ©ponses de Brain de maniÃ¨re naturelle
    - GÃ©nÃ©rer des accusÃ©s de rÃ©ception instantanÃ©s
    """
    config: NeoConfig = field(default_factory=lambda: default_config)
    brain: Optional[Brain] = None
    memory: Optional[MemoryAgent] = None
    conversation_history: list = field(default_factory=list)
    _agent_statuses: dict[str, AgentStatus] = field(default_factory=dict)
    _llm: Optional[object] = None
    _force_mock: bool = False
    _model_config: Optional[object] = None
    _on_thinking_callback: Optional[object] = None  # Callable[[str], None]
    _on_brain_done_callback: Optional[object] = None  # Callable[[str], None] â€” async brain result
    _conversation_store: Optional[ConversationStore] = None
    _current_session: Optional[ConversationSession] = None
    _session_lock: Optional[object] = None  # threading.Lock
    _brain_task: Optional[object] = None  # asyncio.Task â€” tÃ¢che Brain en cours
    _brain_busy: bool = False  # True quand Brain travaille en arriÃ¨re-plan

    def __post_init__(self):
        self._agent_statuses = {
            "Vox": AgentStatus(name="Vox", active=True, current_task="communication"),
            "Brain": AgentStatus(name="Brain"),
            "Memory": AgentStatus(name="Memory"),
        }
        self._model_config = get_agent_model("vox")

        # Lock pour protÃ©ger _current_session contre les accÃ¨s concurrents
        import threading
        self._session_lock = threading.Lock()

        # Initialiser le conversation store (chemin relatif au data_dir de la config)
        db_path = self.config.memory.storage_path / "conversations.db"
        self._conversation_store = ConversationStore(db_path)

        if not self.config.is_mock_mode():
            self._init_llm()

    @property
    def _mock_mode(self) -> bool:
        """VÃ©rifie dynamiquement si Vox est en mode mock.

        Re-vÃ©rifie la config Ã  chaque appel. Si la clÃ© API devient
        disponible aprÃ¨s le dÃ©marrage, Vox s'auto-initialise.
        """
        if self._force_mock:
            return True
        if self.config.is_mock_mode():
            return True
        # La clÃ© est disponible mais le LLM n'est pas encore initialisÃ©
        if self._llm is None and not is_oauth_token(self.config.llm.api_key or ""):
            try:
                self._init_llm()
            except Exception:
                pass  # Vox fonctionne en passthrough sans LLM
        return False

    def _init_llm(self) -> None:
        """Initialise le LLM dÃ©diÃ© de Vox (Haiku â€” rapide et lÃ©ger)."""
        try:
            api_key = self.config.llm.api_key
            if is_oauth_token(api_key):
                # En mode OAuth, Vox utilise les appels directs httpx
                self._llm = None  # Sera gÃ©rÃ© via _vox_llm_call
            else:
                from langchain_anthropic import ChatAnthropic
                self._llm = ChatAnthropic(
                    model=self._model_config.model,
                    api_key=api_key,
                    temperature=self._model_config.temperature,
                    max_tokens=self._model_config.max_tokens,
                )
            logger.info("LLM initialisÃ© : %s", self._model_config.model)
        except Exception as e:
            logger.error("LLM non disponible (%s), mode passthrough", e)

    async def _vox_llm_call(self, prompt: str) -> str:
        """
        Appel LLM dÃ©diÃ© pour Vox (reformulation/restitution).

        Route via le systÃ¨me multi-provider (Ollama, Groq, Gemini, Anthropic).
        Fallback automatique vers Anthropic direct si aucun provider configurÃ©.
        """
        if self._mock_mode:
            return prompt  # Passthrough en mock

        try:
            from neo_core.brain.providers.router import route_chat

            response = await route_chat(
                agent_name="vox",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self._model_config.max_tokens,
                temperature=self._model_config.temperature,
            )

            if response.text and not response.text.startswith("[Erreur"):
                return response.text
            # Fallback : retourne le prompt original
            return prompt

        except Exception as e:
            # Fallback LangChain legacy
            logger.debug("route_chat failed, trying legacy LangChain: %s", e)
            if self._llm:
                try:
                    result = await self._llm.ainvoke(prompt)
                    return result.content
                except Exception as e:
                    logger.debug("LangChain ainvoke failed: %s", e)

        return prompt  # Passthrough si pas de LLM

    def connect(self, brain: Brain, memory: MemoryAgent) -> None:
        """Connecte Vox aux autres agents du Neo Core."""
        self.brain = brain
        self.memory = memory

    def start_new_session(self, user_name: str = "User") -> ConversationSession:
        """DÃ©marre une nouvelle session de conversation."""
        if not self._conversation_store:
            return None
        with self._session_lock:
            self._current_session = self._conversation_store.start_session(user_name)
            logger.info("Started new conversation session: %s", self._current_session.session_id)
            return self._current_session

    def resume_session(self, session_id: str) -> Optional[ConversationSession]:
        """Charge une session existante et restaure son historique (thread-safe)."""
        if not self._conversation_store:
            return None
        with self._session_lock:
            session = self._conversation_store.get_session_by_id(session_id)
            if not session:
                logger.warning("Session not found: %s", session_id)
                return None

            # Charger l'historique
            history = self._conversation_store.get_history(session_id, limit=10000)
            self.conversation_history = []
            for turn in history:
                if turn.role == "human":
                    self.conversation_history.append(HumanMessage(content=turn.content))
                else:
                    self.conversation_history.append(AIMessage(content=turn.content))

            self._current_session = session
            logger.info("Resumed session: %s with %d messages", session_id, len(history))
            return self._current_session

    def get_session_info(self) -> Optional[dict]:
        """Retourne les infos de la session courante."""
        if not self._current_session:
            return None
        return {
            "session_id": self._current_session.session_id,
            "user_name": self._current_session.user_name,
            "created_at": self._current_session.created_at,
            "updated_at": self._current_session.updated_at,
            "message_count": self._current_session.message_count,
        }

    def set_thinking_callback(self, callback) -> None:
        """
        DÃ©finit un callback appelÃ© quand Vox envoie un ack
        pendant que Brain rÃ©flÃ©chit.

        Le callback reÃ§oit un str (message d'accusÃ© de rÃ©ception).
        """
        self._on_thinking_callback = callback

    def set_brain_done_callback(self, callback) -> None:
        """
        DÃ©finit un callback appelÃ© quand Brain termine en arriÃ¨re-plan.
        Le callback reÃ§oit un str (rÃ©ponse de Brain).
        UtilisÃ© en mode asynchrone pour dÃ©livrer le rÃ©sultat au CLI.
        """
        self._on_brain_done_callback = callback

    def push_message(self, message: str, source: str = "brain") -> None:
        """
        Envoie un message proactif Ã  l'utilisateur sur tous les canaux.

        AppelÃ© par le heartbeat quand Brain dÃ©cide de parler spontanÃ©ment.
        Route vers : TUI (callback), Telegram, et stockage mÃ©moire.
        """
        import logging as _log
        _logger = _log.getLogger(__name__)

        # 1. TUI / frontend via callback
        if self._on_brain_done_callback:
            try:
                self._on_brain_done_callback(f"ðŸ§  {message}")
            except Exception as e:
                _logger.debug("push_message callback failed: %s", e)

        # 2. Telegram
        try:
            from neo_core.infra.registry import core_registry
            core_registry.send_telegram(f"ðŸ§  {message}")
        except Exception as e:
            _logger.debug("push_message telegram failed: %s", e)

        # 3. Stockage mÃ©moire (Brain se souvient de ce qu'il a dit)
        if self.memory and self.memory.is_initialized:
            try:
                self.memory.store_memory(
                    content=f"[Proactif â€” {source}] {message}",
                    source="brain_proactive",
                    tags=["proactive", source],
                    importance=0.6,
                )
            except Exception as e:
                _logger.debug("push_message memory store failed: %s", e)

        _logger.info("[Vox] Message proactif envoyÃ©: %s", message[:80])

    @property
    def is_brain_busy(self) -> bool:
        """Indique si Brain travaille en arriÃ¨re-plan."""
        return self._brain_busy

    def _is_simple_message(self, message: str) -> bool:
        """
        DÃ©termine si un message peut Ãªtre traitÃ© par Vox seul (sans Brain).

        Seuls deux cas restent en fast-path Vox :
        1. Commandes slash (/status, /tasks, etc.) â€” routage structurel
        2. Salutations pures isolÃ©es ("salut", "bonjour", "merci")

        Tout le reste est envoyÃ© Ã  Brain pour classification LLM.
        C'est le LLM qui dÃ©cide, pas des heuristiques.
        """
        msg_lower = message.lower().strip()

        # Commandes slash â†’ routage structurel, pas besoin de Brain
        if msg_lower.startswith("/"):
            return True

        # Salutations pures isolÃ©es (le message n'est QUE Ã§a, sans contexte)
        import re as _re
        pure_greetings = (
            r"^(salut|hello|bonjour|bonsoir|hey|hi|yo|coucou|wesh|slt|"
            r"Ã§a va|ca va|quoi de neuf|"
            r"merci|thanks)"
            r"\s*[!.\?]*$"
        )
        if _re.match(pure_greetings, msg_lower):
            return True

        # Tout le reste â†’ Brain (le LLM dÃ©cide)
        return False

    async def _vox_quick_reply(self, message: str) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse rapide de Vox (sans Brain) pour les messages simples.
        Si Brain travaille en arriÃ¨re-plan, mentionne-le.
        """
        brain_status = ""
        if self._brain_busy:
            brain_status = "\n(Brain travaille encore sur ta demande prÃ©cÃ©dente, je te donnerai la rÃ©ponse dÃ¨s qu'elle est prÃªte.)"

        if self._mock_mode:
            return f"Salut ! Je suis Vox, l'interface de Neo Core. Je suis lÃ  pour t'aider !{brain_status}"

        try:
            import asyncio
            prompt = (
                f"Tu es Vox, l'interface de Neo Core. RÃ©ponds de maniÃ¨re courte et naturelle "
                f"Ã  ce message de l'utilisateur (max 2-3 phrases). "
                f"Si on te demande tes capacitÃ©s, explique ce que Neo peut faire : "
                f"chercher sur internet, Ã©crire du code, analyser des donnÃ©es, rÃ©diger des textes, "
                f"traduire, crÃ©er des tÃ¢ches/projets, mÃ©moriser et apprendre.\n\n"
                f"Message : {message}\n\n"
                f"{'Note: Brain travaille actuellement sur une tÃ¢che en arriÃ¨re-plan.' if self._brain_busy else ''}"
            )
            reply = await asyncio.wait_for(
                self._vox_llm_call(prompt),
                timeout=5.0,
            )
            return reply.strip() + brain_status if reply else f"Je suis lÃ  !{brain_status}"
        except Exception:
            return f"Je suis lÃ , comment puis-je t'aider ?{brain_status}"

    async def _generate_ack(self, user_message: str) -> str:
        """
        GÃ©nÃ¨re un accusÃ© de rÃ©ception instantanÃ© via LLM.
        Fallback sur un ack statique si le LLM Ã©choue ou est trop lent.
        """
        import random

        if self._mock_mode:
            return random.choice(STATIC_ACKS)

        try:
            import asyncio
            prompt = VOX_ACK_PROMPT.format(user_message=user_message[:100])

            # Timeout court pour ne pas bloquer â€” l'ack doit Ãªtre rapide
            ack = await asyncio.wait_for(
                self._vox_llm_call(prompt),
                timeout=3.0,
            )

            if ack and len(ack.strip()) < 100:
                return ack.strip()

        except asyncio.TimeoutError as e:
            logger.debug("Acknowledgment generation timeout: %s", e)
        except Exception as e:
            logger.debug("Acknowledgment generation failed: %s", e)

        return random.choice(STATIC_ACKS)

    def _get_personality_injection(self) -> str:
        """RÃ©cupÃ¨re l'injection de personnalitÃ© depuis le PersonaEngine."""
        if self.memory and self.memory.persona_engine and self.memory.persona_engine.is_initialized:
            try:
                return self.memory.persona_engine.get_vox_injection()
            except Exception as e:
                logger.debug("Failed to get personality injection: %s", e)
        return ""

    def get_system_status(self) -> str:
        """GÃ©nÃ¨re un rÃ©sumÃ© de l'Ã©tat de tous les agents."""
        lines = [status.to_string() for status in self._agent_statuses.values()]
        return "\n".join(lines)

    def update_agent_status(self, name: str, active: bool = False,
                            task: Optional[str] = None, progress: float = 0.0) -> None:
        """Met Ã  jour le statut d'un agent."""
        if name in self._agent_statuses:
            self._agent_statuses[name].active = active
            self._agent_statuses[name].current_task = task
            self._agent_statuses[name].progress = progress

    def format_request(self, human_message: str) -> str:
        """
        Reformule et structure la demande humaine.
        Version synchrone (fallback) â€” retourne tel quel.
        """
        return human_message

    def _build_recent_context(self, max_turns: int = 8) -> str:
        """
        Construit un rÃ©sumÃ© des derniers Ã©changes pour contextualiser
        les messages courts ou ambigus (pronoms, rÃ©fÃ©rences implicites).
        """
        if not self.conversation_history:
            return ""

        # Prendre les derniers Ã©changes (max_turns * 2 messages)
        recent = self.conversation_history[-(max_turns * 2):]
        if not recent:
            return ""

        lines = ["Contexte rÃ©cent de la conversation :"]
        for msg in recent:
            role = "User" if isinstance(msg, HumanMessage) else "Neo"
            # Tronquer les rÃ©ponses longues pour ne pas saturer le prompt
            content = msg.content[:400] + "..." if len(msg.content) > 400 else msg.content
            lines.append(f"  {role}: {content}")
        lines.append("")  # Ligne vide avant la demande
        return "\n".join(lines)

    async def format_request_async(self, human_message: str) -> str:
        """
        Reformule intelligemment la demande via le LLM de Vox (Haiku).
        Injecte le contexte rÃ©cent pour rÃ©soudre les pronoms et rÃ©fÃ©rences.
        Si le LLM n'est pas disponible, retourne le message tel quel.
        """
        if self._mock_mode or (not self._llm and not is_oauth_token(self.config.llm.api_key or "")):
            return human_message

        try:
            # Injecter le contexte rÃ©cent pour les messages courts/ambigus ou avec pronoms
            recent_context = ""
            msg_lower = human_message.lower()
            _has_pronoun = any(p in msg_lower for p in _AMBIGUOUS_PRONOUNS)
            if (len(human_message.split()) < 20 or _has_pronoun) and self.conversation_history:
                recent_context = self._build_recent_context()

            prompt = VOX_REFORMULATE_PROMPT.format(
                user_message=human_message,
                recent_context=recent_context,
            )
            result = await self._vox_llm_call(prompt)
            return result.strip() if result else human_message
        except Exception as e:
            logger.debug("Failed to reformat request: %s", e)
            return human_message

    async def process_message(self, human_message: str) -> str:
        """
        Pipeline principal : humain â†’ Vox(LLM) â†’ Brain(LLM) â†’ humain.

        Mode synchrone (par dÃ©faut) :
        1. Si message simple â†’ Vox rÃ©pond directement (sans Brain)
        2. Si message complexe â†’ Vox reformule â†’ Brain traite â†’ retourne rÃ©ponse

        Mode asynchrone (si _on_brain_done_callback est dÃ©fini) :
        1. Si message simple â†’ Vox rÃ©pond immÃ©diatement
        2. Si message complexe â†’ Lance Brain en arriÃ¨re-plan, retourne un ack
        3. Quand Brain termine â†’ callback avec la rÃ©ponse
        """
        # Input validation
        try:
            human_message = validate_message(human_message)
        except ValidationError as e:
            logger.error("Message validation failed: %s", e)
            return f"[Erreur] Message invalide : {e}"

        if not self.brain:
            return "[Erreur] Brain n'est pas connectÃ© Ã  Vox."

        # Enregistre le message dans l'historique (bornÃ© Ã  50 messages max)
        self.conversation_history.append(HumanMessage(content=human_message))
        if len(self.conversation_history) > 50:
            self.conversation_history = self.conversation_history[-50:]

        # â”€â”€ Message simple â†’ Vox rÃ©pond seul (instantanÃ©) â”€â”€
        if self._is_simple_message(human_message):
            reply = await self._vox_quick_reply(human_message)
            self.conversation_history.append(AIMessage(content=reply))
            self._save_conversation_turn(human_message, reply)
            return reply

        # â”€â”€ Message complexe â†’ Brain nÃ©cessaire â”€â”€
        self.update_agent_status("Vox", active=True, task="reformulation", progress=0.3)

        # Skip reformulation sauf pour les messages trÃ¨s courts ET ambigus
        # La reformulation dilue l'intention â†’ Brain reÃ§oit le message original
        msg_lower = human_message.lower()
        _truly_ambiguous = (
            len(human_message.split()) < 5
            and any(p in msg_lower for p in ("Ã§a", "cela", "pareil", "mÃªme chose", "continue", "la suite", "fais-le", "fais le"))
        )
        if _truly_ambiguous and self.conversation_history:
            formatted_request = await self.format_request_async(human_message)
            logger.info("[VOX] Reformulation applied (short ambiguous: '%s')", human_message[:40])
        else:
            logger.info("[VOX] Reformulation skipped â€” message passed directly to Brain (%d chars)", len(human_message))
            formatted_request = human_message

        # Mode asynchrone : lancer Brain en arriÃ¨re-plan
        if self._on_brain_done_callback:
            import asyncio

            # AccusÃ© de rÃ©ception
            ack = await self._generate_ack(human_message)
            self.update_agent_status("Vox", active=False, task="communication", progress=0.0)
            self.update_agent_status("Brain", active=True, task="analyse de la requÃªte", progress=0.5)

            # Lancer Brain en arriÃ¨re-plan
            self._brain_busy = True
            self._brain_task = asyncio.create_task(
                self._brain_background(formatted_request, human_message)
            )

            return ack

        # Mode synchrone (fallback / API / Telegram) : attendre Brain
        if self._on_thinking_callback:
            try:
                ack = await self._generate_ack(human_message)
                self._on_thinking_callback(ack)
            except Exception as e:
                logger.debug("Failed to send thinking callback: %s", e)

        self.update_agent_status("Vox", active=False, task="communication", progress=0.0)
        self.update_agent_status("Brain", active=True, task="analyse de la requÃªte", progress=0.5)

        brain_response = await self.brain.process(
            request=formatted_request,
            conversation_history=self.conversation_history,
            original_request=human_message,
        )

        self.update_agent_status("Brain", active=False, task=None, progress=0.0)
        final_response = brain_response

        self.conversation_history.append(AIMessage(content=final_response))
        self._save_conversation_turn(human_message, final_response)
        self._store_in_memory(human_message, final_response)

        return final_response

    async def _brain_background(self, formatted_request: str, original_message: str) -> None:
        """ExÃ©cute Brain en arriÃ¨re-plan et dÃ©livre le rÃ©sultat via callback."""
        try:
            brain_response = await self.brain.process(
                request=formatted_request,
                conversation_history=self.conversation_history,
                original_request=original_message,
            )

            self.update_agent_status("Brain", active=False, task=None, progress=0.0)

            # Enregistrer dans l'historique
            self.conversation_history.append(AIMessage(content=brain_response))
            self._save_conversation_turn(original_message, brain_response)
            self._store_in_memory(original_message, brain_response)

            # DÃ©livrer via callback
            if self._on_brain_done_callback:
                self._on_brain_done_callback(brain_response)

        except Exception as e:
            logger.error("Brain background task failed: %s", e)
            error_msg = f"[Erreur Brain] {type(e).__name__}: {str(e)[:300]}"
            if self._on_brain_done_callback:
                self._on_brain_done_callback(error_msg)
        finally:
            self._brain_busy = False
            self._brain_task = None

    def _save_conversation_turn(self, human_message: str, response: str) -> None:
        """Sauvegarde un Ã©change dans le ConversationStore."""
        if self._conversation_store and self._current_session:
            try:
                self._conversation_store.append_turn(
                    self._current_session.session_id,
                    "human",
                    human_message,
                )
                self._conversation_store.append_turn(
                    self._current_session.session_id,
                    "assistant",
                    response,
                )
            except Exception as e:
                logger.error("Failed to save conversation turn: %s", e)

    def _store_in_memory(self, human_message: str, response: str) -> None:
        """Stocke l'Ã©change en mÃ©moire persistante et met Ã  jour la working memory."""
        if self.memory and self.memory.is_initialized:
            self.update_agent_status("Memory", active=True, task="stockage", progress=0.5)
            self.memory.on_conversation_turn(human_message, response)
            self.update_agent_status("Memory", active=False, task=None, progress=0.0)

    def get_prompt_template(self) -> ChatPromptTemplate:
        """Retourne le template de prompt pour Vox avec personnalitÃ© injectÃ©e."""
        personality = self._get_personality_injection()
        prompt = VOX_SYSTEM_PROMPT.replace("{personality}", personality)
        return ChatPromptTemplate.from_messages([
            ("system", prompt),
            MessagesPlaceholder("conversation_history"),
            ("human", "{input}"),
        ])

    def get_model_info(self) -> dict:
        """Retourne les infos du modÃ¨le utilisÃ© par Vox."""
        return {
            "agent": "Vox",
            "model": self._model_config.model if self._model_config else "none",
            "role": "Interface utilisateur (reformulation/restitution)",
            "has_llm": self._llm is not None or (
                not self._mock_mode and is_oauth_token(self.config.llm.api_key or "")
            ),
        }


def bootstrap() -> Vox:
    """
    Initialise et connecte les 3 agents du Neo Core.
    Retourne l'agent Vox prÃªt Ã  communiquer.
    """
    from neo_core.brain.core import Brain
    from neo_core.memory.agent import MemoryAgent

    config = NeoConfig()

    memory = MemoryAgent(config=config)
    memory.initialize()

    brain = Brain(config=config)
    brain.connect_memory(memory)

    vox = Vox(config=config)
    vox.connect(brain=brain, memory=memory)

    return vox
